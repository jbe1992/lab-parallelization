<!DOCTYPE html>
<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Portal:Machine learning - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"Portal","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":100,"wgPageName":"Portal:Machine_learning","wgTitle":"Machine learning","wgCurRevisionId":887475509,"wgRevisionId":887475509,"wgArticleId":44942806,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Portals with short description","Single-page portals","All portals","Portals with titles not starting with a proper noun","Machine learning","Computing portals"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Portal:Machine_learning","wgRelevantArticleId":44942806,"wgRequestId":"XIuEtApAAEEAAJvtPM0AAAAT","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsReferencePreviews":false,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgWMESchemaEditAttemptStepOversample":false,"wgPoweredByHHVM":true,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q58630879","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true,"oresWikiId":"enwiki","oresBaseUrl":"http://ores.discovery.wmnet:8081/","oresApiVersion":3});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.math.styles":"ready","mediawiki.page.gallery.styles":"ready","ext.categoryTree.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["ext.math.scripts","mediawiki.page.gallery.slideshow","ext.categoryTree","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.categoryTree.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.page.gallery.styles%7Cmediawiki.skinning.interface%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.33.0-wmf.21" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<link href="/w/index.php?title=Portal:Machine_learning&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Portal:Machine_learning&amp;action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" rel="alternate" title="Wikipedia Atom feed" type="application/atom+xml"/>
<link href="https://en.wikipedia.org/wiki/Portal:Machine_learning" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
<!--[if lt IE 9]><script src="/w/load.php?lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-100 ns-subject mw-editable page-Portal_Machine_learning rootpage-Portal_Machine_learning skin-vector action-view"> <div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Portal:Machine learning</h1> <div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div> <div id="contentSub"></div>
<div id="jump-to-nav"></div> <a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#p-search">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Wikipedia's portal for exploring content related to Machine learning</div>
<div class="portal-maintenance-status" style="display:none;">
<table class="plainlinks ombox ombox-notice" role="presentation"><tbody><tr><td class="mbox-image"><a class="image" href="/wiki/File:Darkgreen_flag_waving.svg"><img alt="Darkgreen flag waving.svg" data-file-height="268" data-file-width="249" decoding="async" height="32" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/30px-Darkgreen_flag_waving.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/45px-Darkgreen_flag_waving.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/60px-Darkgreen_flag_waving.svg.png 2x" width="30"/></a></td><td class="mbox-text"><span style="font-size:108%;"><b>Portal maintenance status:</b></span> <small>(September 2018)</small>
<ul><li>This portal has a <b>single page layout</b>. Any <a href="/wiki/Special:PrefixIndex/Portal:Machine_learning/" title="Special:PrefixIndex/Portal:Machine learning/">subpages</a> are likely no longer needed.</li></ul>
<span style="font-size:90%;">Please <a class="mw-redirect" href="/wiki/Wikipedia:CAREFUL" title="Wikipedia:CAREFUL">take care</a> when editing, especially if using <a class="mw-redirect" href="/wiki/Wikipedia:ASSISTED" title="Wikipedia:ASSISTED">automated editing software</a>. Learn how to <a href="/wiki/Template:Portal_maintenance_status#How_to_update_the_maintenance_information_for_a_portal" title="Template:Portal maintenance status">update the maintenance information here</a>.</span></td></tr></tbody></table></div>
<div class="hlist noprint" id="portals-browsebar" style="text-align: center;">
<dl><dt><a href="/wiki/Portal:Contents/Portals" title="Portal:Contents/Portals">Portal topics</a></dt>
<dd><a href="/wiki/Portal:Contents/Portals#Human_activities" title="Portal:Contents/Portals">Activities</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Culture_and_the_arts" title="Portal:Contents/Portals">Culture</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Geography_and_places" title="Portal:Contents/Portals">Geography</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Health_and_fitness" title="Portal:Contents/Portals">Health</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#History_and_events" title="Portal:Contents/Portals">History</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Mathematics_and_logic" title="Portal:Contents/Portals">Mathematics</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Natural_and_physical_sciences" title="Portal:Contents/Portals">Nature</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#People_and_self" title="Portal:Contents/Portals">People</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Philosophy_and_thinking" title="Portal:Contents/Portals">Philosophy</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Religion_and_belief_systems" title="Portal:Contents/Portals">Religion</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Society_and_social_sciences" title="Portal:Contents/Portals">Society</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Technology_and_applied_sciences" title="Portal:Contents/Portals">Technology</a></dd>
<dd><a href="/wiki/Special:RandomInCategory/All_portals" title="Special:RandomInCategory/All portals">Random portal</a></dd></dl>
</div>
<div style="clear:both; width:100%">
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Introduction">Introduction</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<p><b><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></b> (ML) is the <a href="/wiki/Branches_of_science" title="Branches of science">scientific study</a> of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> and <a href="/wiki/Statistical_model" title="Statistical model">statistical models</a> that <a class="mw-redirect" href="/wiki/Computer_systems" title="Computer systems">computer systems</a> use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>. Machine learning algorithms build a mathematical model of sample data, known as "<a class="mw-redirect" href="/wiki/Training_data" title="Training data">training data</a>", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as <a href="/wiki/Email_filtering" title="Email filtering">email filtering</a>, detection of network intruders, and <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to <a href="/wiki/Computational_statistics" title="Computational statistics">computational statistics</a>, which focuses on making predictions using computers. The study of <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">mathematical optimization</a> delivers methods, theory and application domains to the field of machine learning. <a href="/wiki/Data_mining" title="Data mining">Data mining</a> is a field of study within machine learning, and focuses on <a href="/wiki/Exploratory_data_analysis" title="Exploratory data analysis">exploratory data analysis</a> through <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>. In its application across business problems, machine learning is also referred to as <a href="/wiki/Predictive_analytics" title="Predictive analytics">predictive analytics</a>.
</p>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b><a href="/wiki/Machine_learning" title="Machine learning">Read more...</a></b></div><div style="clear:both;"></div></div>
<div style="text-align:center; margin:0.25em auto 0.75em"><span class="noprint plainlinks purgelink"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;action=purge"><span title="Purge this page"><b>Refresh with new selections below</b> (purge)</span></a></span></div>
<style data-mw-deduplicate="TemplateStyles:r886281085">.mw-parser-output .flex-columns-container{clear:both;width:100%;display:flex;flex-wrap:wrap}.mw-parser-output .flex-columns-container>.flex-columns-column{float:left;width:50%;min-width:360px;padding:0 0.5em;box-sizing:border-box;flex:1;display:flex;flex-direction:column}@media screen and (max-width:393px){.mw-parser-output .flex-columns-container>.flex-columns-column{min-width:0}}.mw-parser-output .flex-columns-container>.flex-columns-column:first-child{padding-left:0}.mw-parser-output .flex-columns-container>.flex-columns-column:last-child{padding-right:0}@media screen and (max-width:720px){.mw-parser-output .flex-columns-container>.flex-columns-column{padding:0;width:100%}.mw-parser-output .flex-columns-container{display:block}}.mw-parser-output .flex-columns-container>.flex-columns-column>div{flex:1 0 auto}.mw-parser-output .flex-columns-container>.flex-columns-column>div.flex-columns-noflex{flex:0}</style><div class="flex-columns-container"><div class="flex-columns-column"><div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Selected_general_articles">Selected general articles</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<style data-mw-deduplicate="TemplateStyles:r886046835">.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div>div>span:nth-child(2){display:none}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div:nth-child(2){display:none}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div:nth-child(1){padding-bottom:0}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li:nth-child(n/**/+2){display:none}.mw-parser-output div.excerptSlideshow-container .gallery .gallerybox,.mw-parser-output div.excerptSlideshow-container .gallery .gallerybox div{width:100%!important;max-width:100%}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li:not(.gallerycarousel)>div>div:nth-child(1){display:none}</style><div class="excerptSlideshow-container" style="max-width:100%; margin:-4em auto;"><ul class="gallery mw-gallery-slideshow" data-showthumbnails="">
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></b> is a <a href="/wiki/Statistics" title="Statistics">statistical</a> method used to describe <a href="/wiki/Variance" title="Variance">variability</a> among observed, correlated <a href="/wiki/Variable_(mathematics)" title="Variable (mathematics)">variables</a> in terms of a potentially lower number of unobserved variables called <b>factors</b>. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved <a href="/wiki/Latent_variable" title="Latent variable">latent variables</a>. The observed variables are modelled as <a href="/wiki/Linear_combination" title="Linear combination">linear combinations</a> of the potential factors, plus "<a class="mw-redirect" href="/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">error</a>" terms. Factor analysis aims to find independent latent variables.<br/><br/>It is a theory used in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and related to <a href="/wiki/Data_mining" title="Data mining">data mining</a>. The theory behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in biology, <a href="/wiki/Psychometrics" title="Psychometrics">psychometrics</a>, <a href="/wiki/Personality" title="Personality">personality</a> theories, <a href="/wiki/Marketing" title="Marketing">marketing</a>, <a href="/wiki/Product_management" title="Product management">product management</a>, <a href="/wiki/Operations_research" title="Operations research">operations research</a>, and <a href="/wiki/Finance" title="Finance">finance</a>. Proponents of factor analysis believe that it helps to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality. <b><a href="/wiki/Factor_analysis" title="Factor analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Test_function_and_noisy_data.png"><img alt="" class="thumbimage" data-file-height="901" data-file-width="1201" decoding="async" height="165" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/220px-Test_function_and_noisy_data.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/330px-Test_function_and_noisy_data.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/440px-Test_function_and_noisy_data.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Test_function_and_noisy_data.png" title="Enlarge"></a></div>Function and noisy data.</div></div></div><br/>In <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, the <b><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">bias–variance tradeoff</a></b> is the property of a set of predictive models whereby models with a lower <a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator">bias</a> in <a href="/wiki/Statistical_parameter" title="Statistical parameter">parameter</a> <a href="/wiki/Estimation_theory" title="Estimation theory">estimation</a> have a higher <a href="/wiki/Variance" title="Variance">variance</a> of the parameter estimates across <a href="/wiki/Sample_(statistics)" title="Sample (statistics)">samples</a>, and vice versa. The <b>bias–variance dilemma</b> or <b>problem</b> is the conflict in trying to simultaneously minimize these two sources of <a class="mw-redirect" href="/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">error</a> that prevent <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> algorithms from generalizing beyond their <a class="mw-redirect" href="/wiki/Training_set" title="Training set">training set</a>:<ul><li> The <a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator"><i>bias</i></a> is an error from erroneous assumptions in the learning <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</li><li> The <i><a href="/wiki/Variance" title="Variance">variance</a></i> is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random <a href="/wiki/Noise_(signal_processing)" title="Noise (signal processing)">noise</a> in the training data, rather than the intended outputs (<a href="/wiki/Overfitting" title="Overfitting">overfitting</a>).</li></ul><br/><br/>The <b>bias–variance decomposition</b> is a way of analyzing a learning algorithm's <a href="/wiki/Expected_value" title="Expected value">expected</a> <a href="/wiki/Generalization_error" title="Generalization error">generalization error</a> with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the <i>irreducible error</i>, resulting from noise in the problem itself. <b><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Linear discriminant analysis</a></b> (<b>LDA</b>), <b>normal discriminant analysis</b> (<b>NDA</b>), or <b>discriminant function analysis</b> is a generalization of <b>Fisher's linear discriminant</b>, a method used in <a href="/wiki/Statistics" title="Statistics">statistics</a>, <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> to find a <a href="/wiki/Linear_combination" title="Linear combination">linear combination</a> of <a class="mw-redirect" href="/wiki/Features_(pattern_recognition)" title="Features (pattern recognition)">features</a> that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>, or, more commonly, for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> before later <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>.<br/><br/>LDA is closely related to <a href="/wiki/Analysis_of_variance" title="Analysis of variance">analysis of variance</a> (ANOVA) and <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, which also attempt to express one <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a> as a linear combination of other features or measurements. However, ANOVA uses <a href="/wiki/Categorical_variable" title="Categorical variable">categorical</a> <a class="mw-redirect" href="/wiki/Independent_variables" title="Independent variables">independent variables</a> and a <a class="mw-redirect" href="/wiki/Continuous_variable" title="Continuous variable">continuous</a> <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a>, whereas discriminant analysis has continuous <a class="mw-redirect" href="/wiki/Independent_variables" title="Independent variables">independent variables</a> and a categorical dependent variable (<i>i.e.</i> the class label). <a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a> and <a class="mw-redirect" href="/wiki/Probit_regression" title="Probit regression">probit regression</a> are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method. <b><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> is the problem of identifying to which of a set of <a class="mw-redirect" href="/wiki/Categorical_data" title="Categorical data">categories</a> (sub-populations) a new <a href="/wiki/Observation" title="Observation">observation</a> belongs, on the basis of a <a class="mw-redirect" href="/wiki/Training_set" title="Training set">training set</a> of data containing observations (or instances) whose category membership is known.  Examples are assigning a given email to the <a class="mw-redirect" href="/wiki/Spam_filtering" title="Spam filtering">"spam" or "non-spam"</a> class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).  Classification is an example of <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>.<br/><br/>In the terminology of machine learning, classification is considered an instance of <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, i.e., learning where a training set of correctly identified observations is available.  The corresponding <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> procedure is known as <a href="/wiki/Cluster_analysis" title="Cluster analysis">clustering</a>, and involves grouping data into categories based on some measure of inherent similarity or <a href="/wiki/Distance" title="Distance">distance</a>. <b><a href="/wiki/Statistical_classification" title="Statistical classification">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Canonical_correlation" title="Canonical correlation">canonical-correlation analysis</a></b> (<b>CCA</b>, also called 'Canonical Variates Analysis') is a way of inferring information from <a href="/wiki/Cross-covariance_matrix" title="Cross-covariance matrix">cross-covariance matrices</a>. If we have two vectors <i>X</i> = (<i>X</i><sub>1</sub>, ..., <i>X</i><sub><i>n</i></sub>) and <i>Y</i> = (<i>Y</i><sub>1</sub>, ..., <i>Y</i><sub><i>m</i></sub>)  of <a href="/wiki/Random_variable" title="Random variable">random variables</a>, and there are <a class="mw-redirect" href="/wiki/Correlation" title="Correlation">correlations</a> among the variables, then canonical-correlation analysis will find linear combinations of <i>X</i> and <i>Y</i> which have maximum correlation with each other. T. R. Knapp notes that "virtually all of the commonly encountered <a href="/wiki/Parametric_statistics" title="Parametric statistics">parametric tests</a> of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables." The method was first introduced by <a href="/wiki/Harold_Hotelling" title="Harold Hotelling">Harold Hotelling</a> in 1936, although in the context of <a href="/wiki/Angles_between_flats" title="Angles between flats">angles between flats</a> the mathematical concept was published by Jordan in 1875. <b><a href="/wiki/Canonical_correlation" title="Canonical correlation">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a>, <b><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">probably approximately correct</a></b> (<b>PAC</b>) <b>learning</b> is a framework for mathematical analysis of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>. It was proposed in 1984 by <a href="/wiki/Leslie_Valiant" title="Leslie Valiant">Leslie Valiant</a>.<br/><br/>In this framework, the learner receives samples and must select a generalization function (called the <i>hypothesis</i>) from a certain class of possible functions. The goal is that, with high probability (the "probably" part), the selected function will have low <a href="/wiki/Generalization_error" title="Generalization error">generalization error</a> (the "approximately correct" part). The learner must be able to learn the concept given any arbitrary approximation ratio, probability of success, or <a href="/wiki/Empirical_distribution_function" title="Empirical distribution function">distribution of the samples</a>. <b><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><a href="/wiki/Feature_learning" title="Feature learning">feature learning</a></b> or <b>representation learning</b> is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual <a href="/wiki/Feature_engineering" title="Feature engineering">feature engineering</a> and allows a machine to both learn the features  and use them to perform  a specific task.<br/><br/>Feature learning is motivated by the fact that machine learning tasks such as <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. <b><a href="/wiki/Feature_learning" title="Feature learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:352px;"><a class="image" href="/wiki/File:Autoencoder_structure.png"><img alt="" class="thumbimage" data-file-height="506" data-file-width="677" decoding="async" height="262" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/525px-Autoencoder_structure.png 1.5x, //upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png 2x" width="350"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Autoencoder_structure.png" title="Enlarge"></a></div>Schematic structure of an autoencoder with 3 fully connected hidden layers.</div></div></div><br/>An <b><a href="/wiki/Autoencoder" title="Autoencoder">autoencoder</a></b> is a type of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> used to learn <a href="/wiki/Feature_learning" title="Feature learning">efficient data codings</a> in an <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a>, by training the network to ignore signal “noise.” Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Recently, the autoencoder concept has become more widely used for learning <a href="/wiki/Generative_model" title="Generative model">generative models</a> of data. Some of the most powerful AI in the 2010s have involved sparse autoencoders stacked inside of deep neural networks. <b><a href="/wiki/Autoencoder" title="Autoencoder">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></b> (<b>RL</b>) is an area of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> concerned with how <a href="/wiki/Software_agent" title="Software agent">software agents</a> ought to take <i><a href="/wiki/Action_selection" title="Action selection">actions</a></i> in an <i>environment</i> so as to maximize some notion of cumulative <i>reward</i>. The problem, due to its generality, is studied in many other disciplines, such as <a href="/wiki/Game_theory" title="Game theory">game theory</a>, <a href="/wiki/Control_theory" title="Control theory">control theory</a>, <a href="/wiki/Operations_research" title="Operations research">operations research</a>, <a href="/wiki/Information_theory" title="Information theory">information theory</a>, <a href="/wiki/Simulation-based_optimization" title="Simulation-based optimization">simulation-based optimization</a>, <a href="/wiki/Multi-agent_system" title="Multi-agent system">multi-agent systems</a>, <a href="/wiki/Swarm_intelligence" title="Swarm intelligence">swarm intelligence</a>, <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Genetic_algorithm" title="Genetic algorithm">genetic algorithms</a>. In the operations research and control literature, reinforcement learning is called <i>approximate dynamic programming,</i> or <i>neuro-dynamic programming.</i><br/>The problems of interest in reinforcement learning have also been studied in the <a class="mw-redirect" href="/wiki/Optimal_control_theory" title="Optimal control theory">theory of optimal control</a>, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In <a href="/wiki/Economics" title="Economics">economics</a> and <a href="/wiki/Game_theory" title="Game theory">game theory</a>, reinforcement learning may be used to explain how equilibrium may arise under <a href="/wiki/Bounded_rationality" title="Bounded rationality">bounded rationality</a>.<br/>In machine learning, the environment is typically formulated as a <a class="mw-redirect" href="/wiki/Markov_Decision_Process" title="Markov Decision Process">Markov Decision Process</a> (MDP), as many reinforcement learning algorithms for this context utilize <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a> techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.<br/><br/>Reinforcement learning is considered as one of three machine learning paradigms, alongside <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> and <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>. It differs from supervised learning in that correct input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance, which involves finding a balance between <a href="/wiki/Exploration" title="Exploration">exploration</a> (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off has been most thoroughly studied through the <a href="/wiki/Multi-armed_bandit" title="Multi-armed bandit">multi-armed bandit</a> problem and in finite MDPs. <b><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bootstrap aggregating</a></b>, also called <b>bagging</b>, is a <a href="/wiki/Ensemble_learning" title="Ensemble learning">machine learning ensemble</a> <a class="mw-redirect" href="/wiki/Meta-algorithm" title="Meta-algorithm">meta-algorithm</a> designed to improve the stability and accuracy of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms used in <a href="/wiki/Statistical_classification" title="Statistical classification">statistical classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>. It also reduces <a href="/wiki/Variance" title="Variance">variance</a> and helps to avoid <a href="/wiki/Overfitting" title="Overfitting">overfitting</a>. Although it is usually applied to <a href="/wiki/Decision_tree_learning" title="Decision tree learning">decision tree</a> methods, it can be used with any type of method. Bagging is a special case of the <a href="/wiki/Ensemble_learning" title="Ensemble learning">model averaging</a> approach. <b><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><i>Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself.  However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together.  You can help enhance this page by adding new terms or writing definitions for existing ones.</i><br/><br/>This <b>glossary of artificial intelligence terms</b> is about <b><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></b>, its sub-disciplines, and related fields. <b><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Mathematics" title="Mathematics">mathematics</a>, a <b><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance Vector Machine (RVM)</a></b> is a <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> technique that uses <a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a> to obtain <a href="/wiki/Occam%27s_razor" title="Occam's razor">parsimonious</a> solutions for <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> and <a href="/wiki/Probabilistic_classification" title="Probabilistic classification">probabilistic classification</a>.<br/>The RVM has an identical functional form to the <a class="mw-redirect" href="/wiki/Support_vector_machine" title="Support vector machine">support vector machine</a>, but provides probabilistic classification.<br/><br/>It is actually equivalent to a <a href="/wiki/Gaussian_process" title="Gaussian process">Gaussian process</a> model with <a href="/wiki/Covariance_function" title="Covariance function">covariance function</a>:<br/>:<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>N</mi>
</mrow>
</munderover>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<msub>
<mi>α<!-- α --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
</mfrac>
</mrow>
<mi>φ<!-- φ --></mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mi>φ<!-- φ --></mi>
<mo stretchy="false">(</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>′</mo>
</msup>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}</annotation>
</semantics>
</math></span><img alt="k(\mathbf{x},\mathbf{x'}) = \sum_{j=1}^N \frac{1}{\alpha_j} \varphi(\mathbf{x},\mathbf{x}_j)\varphi(\mathbf{x}',\mathbf{x}_j) " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a9b6842d3fb3de1fc0b5cd93878ce056ca7997f" style="vertical-align: -3.338ex; width:34.51ex; height:7.676ex;"/></span><br/>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \varphi }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>φ<!-- φ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \varphi }</annotation>
</semantics>
</math></span><img alt="\varphi " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/33ee699558d09cf9d653f6351f9fda0b2f4aaa3e" style="vertical-align: -0.838ex; width:1.52ex; height:2.176ex;"/></span> is the <a class="mw-redirect" href="/wiki/Kernel_function" title="Kernel function">kernel function</a> (usually Gaussian), <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \alpha _{j}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>α<!-- α --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \alpha _{j}}</annotation>
</semantics>
</math></span><img alt="\alpha _{j}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/293a364991ab1ee55c25b0f60fd9e52af7b7dbde" style="vertical-align: -1.005ex; width:2.397ex; height:2.343ex;"/></span> are the variances of the prior on the weight vector<br/><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle w\sim N(0,\alpha ^{-1}I)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>w</mi>
<mo>∼<!-- ∼ --></mo>
<mi>N</mi>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<msup>
<mi>α<!-- α --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msup>
<mi>I</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle w\sim N(0,\alpha ^{-1}I)}</annotation>
</semantics>
</math></span><img alt="w \sim N(0,\alpha^{-1}I)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f4348b3f595d38d41a8dee315d073be107af558" style="vertical-align: -0.838ex; width:15.824ex; height:3.176ex;"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>N</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}</annotation>
</semantics>
</math></span><img alt="\mathbf{x}_1,\ldots,\mathbf{x}_N" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ceacccf86726a1ba6acfd40e67538e3e7c2336ec" style="vertical-align: -0.671ex; width:10.746ex; height:2.009ex;"/></span> are the input vectors of the <a class="mw-redirect" href="/wiki/Training_set" title="Training set">training set</a>. <b><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></b> is a <a class="mw-redirect" href="/wiki/Non-parametric" title="Non-parametric">non-parametric</a> <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature-space</a> analysis technique for locating the maxima of a <a class="mw-redirect" href="/wiki/Density_function" title="Density function">density function</a>, a so-called <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">mode</a>-seeking algorithm. Application domains include <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> and <a class="mw-redirect" href="/wiki/Image_processing" title="Image processing">image processing</a>. <b><a href="/wiki/Mean_shift" title="Mean shift">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, the <b><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-nearest neighbors algorithm</a></b> (<b><i>k</i>-NN</b>) is a <a class="mw-redirect" href="/wiki/Non-parametric_statistics" title="Non-parametric statistics">non-parametric</a> method used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>. In both cases, the input consists of the <i>k</i> closest training examples in the <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature space</a>. The output depends on whether <i>k</i>-NN is used for classification or regression:<br/><br/>:* In <i>k-NN classification</i>, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its <i>k</i> nearest neighbors (<i>k</i> is a positive <a href="/wiki/Integer" title="Integer">integer</a>, typically small). If <i>k</i> = 1, then the object is simply assigned to the class of that single nearest neighbor. <b><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>, the <b><a href="/wiki/Local_outlier_factor" title="Local outlier factor">local outlier factor</a></b> (<b>LOF</b>) is an algorithm proposed by Markus M. Breunig, <a href="/wiki/Hans-Peter_Kriegel" title="Hans-Peter Kriegel">Hans-Peter Kriegel</a>, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.<br/><br/>LOF shares some concepts with <a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a> and <a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a> such as the concepts of "core distance" and "reachability distance", which are used for local density estimation. <b><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">T-distributed Stochastic Neighbor Embedding (t-SNE)</a></b> is a <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithm for <a href="/wiki/Data_visualization" title="Data visualization">visualization</a> developed by Laurens van der Maaten and <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a>. It is a <a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">nonlinear dimensionality reduction</a> technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.<br/><br/>The t-SNE algorithm comprises two main stages. First, t-SNE constructs a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the <a href="/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback–Leibler divergence">Kullback–Leibler divergence</a> between the two distributions with respect to the locations of the points in the map. Note that while the original algorithm uses the <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> between objects as the base of its similarity metric, this should be changed as appropriate. <b><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Linear_regression" title="Linear regression">linear regression</a></b> is a <a href="/wiki/Linearity" title="Linearity">linear</a> approach to modelling the relationship between a scalar response (or <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a>) and one or more <a class="mw-redirect" href="/wiki/Explanatory_variable" title="Explanatory variable">explanatory variables</a> (or <a class="mw-redirect" href="/wiki/Independent_variable" title="Independent variable">independent variables</a>). The case of one explanatory variable is called <a href="/wiki/Simple_linear_regression" title="Simple linear regression">simple linear regression</a>. For more than one explanatory variable, the process is called <b>multiple linear regression</b>. This term is distinct from <a class="mw-redirect" href="/wiki/Multivariate_linear_regression" title="Multivariate linear regression">multivariate linear regression</a>, where multiple correlated dependent variables are predicted, rather than a single scalar variable.<br/><br/>In linear regression, the relationships are modeled using <a href="/wiki/Linear_predictor_function" title="Linear predictor function">linear predictor functions</a> whose unknown model <a class="mw-redirect" href="/wiki/Parameters" title="Parameters">parameters</a> are <a href="/wiki/Estimation_theory" title="Estimation theory">estimated</a> from the <a href="/wiki/Data" title="Data">data</a>. Such models are called <a href="/wiki/Linear_model" title="Linear model">linear models</a>. Most commonly, the <a href="/wiki/Conditional_expectation" title="Conditional expectation">conditional mean</a> of the response given the values of the explanatory variables (or predictors) is assumed to be an <a href="/wiki/Affine_transformation" title="Affine transformation">affine function</a> of those values; less commonly, the conditional <a href="/wiki/Median" title="Median">median</a> or some other <a href="/wiki/Quantile" title="Quantile">quantile</a> is used. Like all forms of <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, linear regression focuses on the <a href="/wiki/Conditional_probability_distribution" title="Conditional probability distribution">conditional probability distribution</a> of the response given the values of the predictors, rather than on the <a href="/wiki/Joint_probability_distribution" title="Joint probability distribution">joint probability distribution</a> of all of these variables, which is the domain of <a href="/wiki/Multivariate_analysis" title="Multivariate analysis">multivariate analysis</a>. <b><a href="/wiki/Linear_regression" title="Linear regression">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov Model</a></b> (<b>HMM</b>) is a <a href="/wiki/Statistical_model" title="Statistical model">statistical</a> <a href="/wiki/Markov_model" title="Markov model">Markov model</a> in which the system being modeled is assumed to be a <a class="mw-redirect" href="/wiki/Markov_process" title="Markov process">Markov process</a> with unobserved (i.e. <i>hidden</i>) states.<br/><br/>The hidden Markov model can be represented as the simplest <a href="/wiki/Dynamic_Bayesian_network" title="Dynamic Bayesian network">dynamic Bayesian network</a>. The mathematics behind the HMM were developed by <a href="/wiki/Leonard_E._Baum" title="Leonard E. Baum">L. E. Baum</a> and coworkers.<br/> HMM is closely related to earlier work on the optimal nonlinear <a href="/wiki/Filtering_problem_(stochastic_processes)" title="Filtering problem (stochastic processes)">filtering problem</a> by <a class="mw-redirect" href="/wiki/Ruslan_L._Stratonovich" title="Ruslan L. Stratonovich">Ruslan L. Stratonovich</a>, who was the first to describe the <a href="/wiki/Forward%E2%80%93backward_algorithm" title="Forward–backward algorithm">forward-backward procedure</a>. <b><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistical_model" title="Statistical model">statistical modeling</a>, <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a></b> is a set of statistical processes for <a href="/wiki/Estimation_theory" title="Estimation theory">estimating</a> the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a> and one or more <a class="mw-redirect" href="/wiki/Independent_variable" title="Independent variable">independent variables</a> (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable  (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.<br/><br/>Most commonly, regression analysis estimates the <a href="/wiki/Conditional_expectation" title="Conditional expectation">conditional expectation</a> of the dependent variable given the independent variables – that is, the <a class="mw-redirect" href="/wiki/Average_value" title="Average value">average value</a> of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a <a href="/wiki/Quantile" title="Quantile">quantile</a>, or other <a href="/wiki/Location_parameter" title="Location parameter">location parameter</a> of the conditional distribution of the dependent variable given the independent variables. In all cases, a <a href="/wiki/Function_(mathematics)" title="Function (mathematics)">function</a> of the independent variables called the <b>regression function</b> is to be estimated. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the prediction of the  regression function using a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a>. A related but distinct approach is Necessary Condition Analysis (NCA), which estimates the maximum (rather than average) value of the dependent variable for a given value of the independent variable (ceiling line rather than central line) in order to identify what value of the independent variable is <a href="/wiki/Necessity_and_sufficiency" title="Necessity and sufficiency">necessary but not sufficient</a> for a given value of the dependent variable. <b><a href="/wiki/Regression_analysis" title="Regression analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/CURE_algorithm" title="CURE algorithm">CURE</a></b> (Clustering Using REpresentatives) is an efficient <a class="mw-redirect" href="/wiki/Data_clustering" title="Data clustering">data clustering</a> algorithm for large <a href="/wiki/Database" title="Database">databases</a>. Compared with <a href="/wiki/K-means_clustering" title="K-means clustering">K-means clustering</a> it is more <a href="/wiki/Robust_statistics" title="Robust statistics">robust</a> to <a href="/wiki/Outlier" title="Outlier">outliers</a> and able to identify clusters having non-spherical shapes and size variances. <b><a href="/wiki/CURE_algorithm" title="CURE algorithm">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b><a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a></b> (or just <b>learning theory</b>) is a subfield of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> devoted to studying the design and analysis of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms. <b><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b><a href="/wiki/Online_machine_learning" title="Online machine learning">online machine learning</a></b> is a method of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of <a class="mw-redirect" href="/wiki/Out-of-core" title="Out-of-core">out-of-core</a> algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., <a href="/wiki/Stock_market_prediction" title="Stock market prediction">stock price prediction</a>.<br/>Online learning algorithms may be prone to <a href="/wiki/Catastrophic_interference" title="Catastrophic interference">catastrophic interference</a>, a problem that can be addressed by <a href="/wiki/Incremental_learning" title="Incremental learning">incremental learning</a> approaches. <b><a href="/wiki/Online_machine_learning" title="Online machine learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><a href="/wiki/Ensemble_learning" title="Ensemble learning">ensemble methods</a></b> use multiple learning algorithms to obtain better <a href="/wiki/Predictive_inference" title="Predictive inference">predictive performance</a> than could be obtained from any of the constituent learning algorithms alone.<br/>Unlike a <a class="mw-redirect" href="/wiki/Statistical_ensemble" title="Statistical ensemble">statistical ensemble</a> in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives. <b><a href="/wiki/Ensemble_learning" title="Ensemble learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></b> is a <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> program created by <a href="/wiki/Google" title="Google">Google</a> engineer Alexander Mordvintsev which uses a <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural network</a> to find and enhance patterns in <a href="/wiki/Image" title="Image">images</a> via <a href="/wiki/Algorithm" title="Algorithm">algorithmic</a> <a href="/wiki/Pareidolia" title="Pareidolia">pareidolia</a>, thus creating a <a href="/wiki/Dream" title="Dream">dream</a>-like <a class="mw-redirect" href="/wiki/Hallucinogenic" title="Hallucinogenic">hallucinogenic</a> appearance in the deliberately over-processed images.<br/><br/>Google's program popularized the term (deep) "dreaming" to refer to the generation of images that produce desired <a class="mw-redirect" href="/wiki/Activation_(neural_network)" title="Activation (neural network)">activations</a> in a trained <a class="mw-redirect" href="/wiki/Deep_neural_network" title="Deep neural network">deep network</a>, and the term now refers to a collection of related approaches. <b><a href="/wiki/DeepDream" title="DeepDream">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Data_mining" title="Data mining">data mining</a>, <b><a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a></b> (also <b>outlier detection</b>) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as <a href="/wiki/Bank_fraud" title="Bank fraud">bank fraud</a>, a structural defect, medical problems or errors in a text. Anomalies are also referred to as <a href="/wiki/Outlier" title="Outlier">outliers</a>, novelties, noise, deviations and exceptions.<br/><br/>In particular, in the context of abuse and network intrusion detection, the interesting objects are often not <i>rare</i> objects, but unexpected <i>bursts</i> in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> algorithm may be able to detect the micro clusters formed by these patterns. <b><a href="/wiki/Anomaly_detection" title="Anomaly detection">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Signal_processing" title="Signal processing">signal processing</a>, <b><a href="/wiki/Independent_component_analysis" title="Independent component analysis">independent component analysis</a></b> (<b>ICA</b>) is a computational method for separating a <a href="/wiki/Multivariate_statistics" title="Multivariate statistics">multivariate</a> signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are <a class="mw-redirect" href="/wiki/Statistical_independence" title="Statistical independence">statistically independent</a> from each other. ICA is a special case of <a class="mw-redirect" href="/wiki/Blind_source_separation" title="Blind source separation">blind source separation</a>. A common example application is the "<a class="mw-redirect" href="/wiki/Cocktail_party_problem" title="Cocktail party problem">cocktail party problem</a>" of listening in on one person's speech in a noisy room. <b><a href="/wiki/Independent_component_analysis" title="Independent component analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:302px;"><a class="image" href="/wiki/File:Colored_neural_network.svg"><img alt="" class="thumbimage" data-file-height="356" data-file-width="296" decoding="async" height="361" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/450px-Colored_neural_network.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/600px-Colored_neural_network.svg.png 2x" width="300"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Colored_neural_network.svg" title="Enlarge"></a></div>An artificial neural network is an interconnected group of nodes, similar to the vast network of <a href="/wiki/Neuron" title="Neuron">neurons</a> in a <a href="/wiki/Brain" title="Brain">brain</a>. Here, each circular node represents an <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neuron</a> and an arrow represents a connection from the output of one artificial neuron to the input of another.</div></div></div><br/><b><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></b> (<b>ANN</b>) or <b>connectionist systems</b> are computing systems inspired by the <a class="mw-redirect" href="/wiki/Biological_neural_network" title="Biological neural network">biological neural networks</a> that constitute animal <a href="/wiki/Brain" title="Brain">brains</a>. The neural network itself is not an algorithm, but rather a framework for many different <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms to work together and process complex data inputs. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules. For example, in <a class="mw-redirect" href="/wiki/Image_recognition" title="Image recognition">image recognition</a>, they might learn to identify images that contain cats by analyzing example images that have been manually <a href="/wiki/Labeled_data" title="Labeled data">labeled</a> as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge about cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the learning material that they process.<br/><br/>An ANN is based on a collection of connected units or nodes called <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neurons</a>, which loosely model the <a href="/wiki/Neuron" title="Neuron">neurons</a> in a biological brain. Each connection, like the <a href="/wiki/Synapse" title="Synapse">synapses</a> in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. <b><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:172px;"><a class="image" href="/wiki/File:The_LSTM_cell.png"><img alt="" class="thumbimage" data-file-height="1322" data-file-width="2014" decoding="async" height="112" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/170px-The_LSTM_cell.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/255px-The_LSTM_cell.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/340px-The_LSTM_cell.png 2x" width="170"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:The_LSTM_cell.png" title="Enlarge"></a></div>The Long Short-Term Memory (LSTM) cell can process data sequentially and keep its hidden state through time.</div></div></div><br/><br/><b><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory</a></b> (<b>LSTM</b>) is an artificial <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> (RNN) architecture used in the field of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>. Unlike standard <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural networks</a>, LSTM has feedback connections that make it a "general purpose computer" (that is, it can compute anything that a <a href="/wiki/Turing_machine" title="Turing machine">Turing machine</a> can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a> or <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>.<br/><a class="mw-redirect" href="/wiki/Bloomberg_Business_Week" title="Bloomberg Business Week">Bloomberg Business Week</a> wrote: "These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music."<br/><br/>A common LSTM unit is composed of a <b>cell</b>, an <b>input gate</b>, an <b>output gate</b> and a <b>forget gate</b>. The cell remembers values over arbitrary time intervals and the three <i>gates</i> regulate the flow of information into and out of the cell. <b><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></b> is a term used for <a class="mw-redirect" href="/wiki/Hebbian_learning" title="Hebbian learning">Hebbian learning</a>, associated to learning without a teacher, also known as <a href="/wiki/Self-organization" title="Self-organization">self-organization</a> and a method of modelling the probability density of inputs. <br/><br/>The <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> as a branch of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. <b><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:GaussianScatterPCA.svg"><img alt="" class="thumbimage" data-file-height="720" data-file-width="720" decoding="async" height="220" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/220px-GaussianScatterPCA.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/330px-GaussianScatterPCA.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/440px-GaussianScatterPCA.svg.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:GaussianScatterPCA.svg" title="Enlarge"></a></div>PCA of a <a class="mw-redirect" href="/wiki/Multivariate_Gaussian_distribution" title="Multivariate Gaussian distribution">multivariate Gaussian distribution</a> centered at (1,3) with a standard deviation of 3 in roughly the (0.866, 0.5) direction and of 1 in the orthogonal direction. The vectors shown are the <a href="/wiki/Eigenvalues_and_eigenvectors" title="Eigenvalues and eigenvectors">eigenvectors</a> of the <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a> scaled by the square root of the corresponding eigenvalue, and shifted so their tails are at the mean.</div></div></div><br/><br/><b><a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a></b> (<b>PCA</b>) is a statistical procedure that uses an <a href="/wiki/Orthogonal_transformation" title="Orthogonal transformation">orthogonal transformation</a> to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of <a href="/wiki/Correlation_and_dependence" title="Correlation and dependence">linearly uncorrelated</a> variables called <b>principal components</b>. If there are <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle n}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>n</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle n}</annotation>
</semantics>
</math></span><img alt="n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span> observations with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>p</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p}</annotation>
</semantics>
</math></span><img alt="p" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;"/></span> variables, then the number of distinct principal components is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \min(n-1,p)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo form="prefix" movablelimits="true">min</mo>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo>,</mo>
<mi>p</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \min(n-1,p)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \min(n-1,p)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/050538805fc5cf2f3205b2b758cba07bc6befc18" style="vertical-align: -0.838ex; width:13.285ex; height:2.843ex;"/></span>. This transformation is defined in such a way that the first principal component has the largest possible <a href="/wiki/Variance" title="Variance">variance</a> (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is <a class="mw-redirect" href="/wiki/Orthogonal" title="Orthogonal">orthogonal</a> to the preceding components. The resulting vectors (each being a <a href="/wiki/Linear_combination" title="Linear combination">linear combination</a> of the variables and containing <i>n</i> observations) are an uncorrelated <a class="mw-redirect" href="/wiki/Orthogonal_basis_set" title="Orthogonal basis set">orthogonal basis set</a>. PCA is sensitive to the relative scaling of the original variables.<br/><br/>PCA was invented in 1901 by <a href="/wiki/Karl_Pearson" title="Karl Pearson">Karl Pearson</a>, as an analogue of the <a href="/wiki/Principal_axis_theorem" title="Principal axis theorem">principal axis theorem</a> in mechanics; it was later independently developed and named by <a href="/wiki/Harold_Hotelling" title="Harold Hotelling">Harold Hotelling</a> in the 1930s. Depending on the field of application, it is also named the discrete <a href="/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem" title="Karhunen–Loève theorem">Karhunen–Loève</a> transform (KLT) in <a href="/wiki/Signal_processing" title="Signal processing">signal processing</a>, the <a href="/wiki/Harold_Hotelling" title="Harold Hotelling">Hotelling</a> transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a> (SVD) of <b>X</b> (Golub and Van Loan, 1983), <a class="mw-redirect" href="/wiki/Eigendecomposition" title="Eigendecomposition">eigenvalue decomposition</a> (EVD) of <b>X</b><sup>T</sup><b>X</b> in linear algebra, <a href="/wiki/Factor_analysis" title="Factor analysis">factor analysis</a> (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's <i>Principal Component Analysis</i>), <a class="mw-redirect" href="/wiki/Eckart%E2%80%93Young_theorem" title="Eckart–Young theorem">Eckart–Young theorem</a> (Harman, 1960), or  <a href="/wiki/Empirical_orthogonal_functions" title="Empirical orthogonal functions">empirical orthogonal functions</a> (EOF) in meteorological science, <a class="new" href="/w/index.php?title=Empirical_eigenfunction_decomposition&amp;action=edit&amp;redlink=1" title="Empirical eigenfunction decomposition (page does not exist)">empirical eigenfunction decomposition</a> (Sirovich, 1987), <a class="new" href="/w/index.php?title=Empirical_component_analysis&amp;action=edit&amp;redlink=1" title="Empirical component analysis (page does not exist)">empirical component analysis</a> (Lorenz, 1956), <a class="new" href="/w/index.php?title=Quasiharmonic_modes&amp;action=edit&amp;redlink=1" title="Quasiharmonic modes (page does not exist)">quasiharmonic modes</a> (Brooks et al., 1988), <a href="/wiki/Spectral_theorem" title="Spectral theorem">spectral decomposition</a> in noise and vibration, and <a class="mw-redirect" href="/wiki/Mode_shape" title="Mode shape">empirical modal analysis</a> in structural dynamics. <b><a href="/wiki/Principal_component_analysis" title="Principal component analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">These <a href="/wiki/Data_set" title="Data set">datasets</a> are used for <a class="mw-redirect" href="/wiki/Machine-learning" title="Machine-learning">machine-learning</a> research and have been cited in <a href="/wiki/Peer_review" title="Peer review">peer-reviewed</a> academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> (such as <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for <a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a> and <a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">semi-supervised</a> machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> learning can also be difficult and costly to produce. <b><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><a href="/wiki/Support-vector_machine" title="Support-vector machine">support-vector machines</a></b> (<b>SVMs</b>, also <b>support-vector networks</b>) are <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> models with associated learning <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> that analyze data used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>.  Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-<a href="/wiki/Probabilistic_classification" title="Probabilistic classification">probabilistic</a> <a class="mw-redirect" href="/wiki/Binary_classifier" title="Binary classifier">binary</a> <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a> (although methods such as <a href="/wiki/Platt_scaling" title="Platt scaling">Platt scaling</a> exist to use SVM in a probabilistic classification setting). A SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.<br/><br/>In addition to performing <a href="/wiki/Linear_classifier" title="Linear classifier">linear classification</a>, SVMs can efficiently perform a non-linear classification using what is called the <a class="mw-redirect" href="/wiki/Kernel_trick" title="Kernel trick">kernel trick</a>, implicitly mapping their inputs into high-dimensional feature spaces. <b><a href="/wiki/Support-vector_machine" title="Support-vector machine">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></b> or <b>structured (output) learning</b> is an <a class="mw-redirect" href="/wiki/Umbrella_term" title="Umbrella term">umbrella term</a> for <a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a> machine learning techniques that involves <a class="mw-redirect" href="/wiki/Predicting" title="Predicting">predicting</a> structured objects, rather than scalar <a href="/wiki/Statistical_classification" title="Statistical classification">discrete</a> or <a href="/wiki/Regression_analysis" title="Regression analysis">real</a> values.<br/><br/>Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and <a href="/wiki/Approximate_inference" title="Approximate inference">approximate inference</a> and learning methods are used. <b><a href="/wiki/Structured_prediction" title="Structured prediction">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a>, <b><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></b> is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to <a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">probably approximately correct (PAC) learning</a>, where the learner is evaluated on its predictive power of a test set.<br/><br/>Occam learnability implies PAC learning, and for a wide variety of <a href="/wiki/Concept_class" title="Concept class">concept classes</a>, the converse is also true: PAC learnability implies Occam learnability. <b><a href="/wiki/Occam_learning" title="Occam learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">The following <a href="/wiki/Outline_(list)" title="Outline (list)">outline</a> is provided as an overview of and topical guide to <b><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">machine learning</a></b>. <a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> is a subfield of <a href="/wiki/Soft_computing" title="Soft computing">soft computing</a> within <a href="/wiki/Computer_science" title="Computer science">computer science</a> that evolved from the study of <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a> in <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>. In 1959, <a href="/wiki/Arthur_Samuel" title="Arthur Samuel">Arthur Samuel</a> defined machine learning as a "field of study that gives computers the ability to learn without being explicitly programmed". Machine learning explores the study and construction of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> that can <a href="/wiki/Learning" title="Learning">learn</a> from and make predictions on <a href="/wiki/Data" title="Data">data</a>. Such algorithms operate by building a <a href="/wiki/Mathematical_model" title="Mathematical model">model</a> from an example <a class="mw-redirect" href="/wiki/Training_set" title="Training set">training set</a> of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions. <b><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">Vapnik–Chervonenkis theory</a></b> (also known as <b>VC theory</b>) was developed during 1960–1990 by <a href="/wiki/Vladimir_Vapnik" title="Vladimir Vapnik">Vladimir Vapnik</a> and <a href="/wiki/Alexey_Chervonenkis" title="Alexey Chervonenkis">Alexey Chervonenkis</a>. The theory is a form of <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a>, which attempts to explain the learning process from a statistical point of view.<br/><br/>VC theory is related to <a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">statistical learning theory</a>  and to <a class="mw-redirect" href="/wiki/Empirical_processes" title="Empirical processes">empirical processes</a>.  <a href="/wiki/Richard_M._Dudley" title="Richard M. Dudley">Richard M. Dudley</a> and <a href="/wiki/Vladimir_Vapnik" title="Vladimir Vapnik">Vladimir Vapnik</a>, among others, have applied VC-theory to <a class="mw-redirect" href="/wiki/Empirical_processes" title="Empirical processes">empirical processes</a>. <b><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Random_forest" title="Random forest">Random forests</a></b> or <b>random decision forests</b> are an <a href="/wiki/Ensemble_learning" title="Ensemble learning">ensemble learning</a> method for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>, <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> and other tasks that operates by constructing a multitude of <a href="/wiki/Decision_tree_learning" title="Decision tree learning">decision trees</a> at training time and outputting the class that is the <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">mode</a> of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of <a href="/wiki/Overfitting" title="Overfitting">overfitting</a> to their <a class="mw-redirect" href="/wiki/Test_set" title="Test set">training set</a>.<br/><br/>The first algorithm for random decision forests was created by <a href="/wiki/Tin_Kam_Ho" title="Tin Kam Ho">Tin Kam Ho</a> using the <a href="/wiki/Random_subspace_method" title="Random subspace method">random subspace method</a>, which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg. <b><a href="/wiki/Random_forest" title="Random forest">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a>, the <b>logistic model</b> (or <b>logit model</b>) is a widely used <a href="/wiki/Statistical_model" title="Statistical model">statistical model</a> that, in its basic form, uses a <a href="/wiki/Logistic_function" title="Logistic function">logistic function</a> to model a <a class="mw-redirect" href="/wiki/Binary_variable" title="Binary variable">binary</a> <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a>; many more complex <a href="#Extensions">extensions</a> exist. In <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, <b><a href="/wiki/Logistic_regression" title="Logistic regression">logistic regression</a></b> (or <b>logit regression</b>) is <a href="/wiki/Estimation_theory" title="Estimation theory">estimating</a> the parameters of a logistic model; it is a form of <a href="/wiki/Binomial_regression" title="Binomial regression">binomial regression</a>. Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail, win/lose, alive/dead or healthy/sick; these are represented by an <a class="mw-redirect" href="/wiki/Indicator_variable" title="Indicator variable">indicator variable</a>, where the two values are labeled "0" and "1". In the logistic model, the <a class="mw-redirect" href="/wiki/Log-odds" title="Log-odds">log-odds</a> (the <a href="/wiki/Logarithm" title="Logarithm">logarithm</a> of the <a href="/wiki/Odds" title="Odds">odds</a>) for the value labeled "1" is a <a href="/wiki/Linear_function_(calculus)" title="Linear function (calculus)">linear combination</a> of one or more <a class="mw-redirect" href="/wiki/Independent_variable" title="Independent variable">independent variables</a> ("predictors"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a <a class="mw-redirect" href="/wiki/Continuous_variable" title="Continuous variable">continuous variable</a> (any real value). The corresponding <a href="/wiki/Probability" title="Probability">probability</a> of the value labeled "1" can vary between 0 (certainly the value "0") and 1 (certainly the value "1"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The <a href="/wiki/Unit_of_measurement" title="Unit of measurement">unit of measurement</a> for the log-odds scale is called a <i><a href="/wiki/Logit" title="Logit">logit</a></i>, from <i><b>log</b>istic un<b>it</b></i>, hence the alternative names. Analogous models with a different <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a> instead of the logistic function can also be used, such as the <a href="/wiki/Probit_model" title="Probit model">probit model</a>; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a <i>constant</i> rate, with each dependent variable having its own parameter; for a binary independent variable this generalizes the <a href="/wiki/Odds_ratio" title="Odds ratio">odds ratio</a>.<br/><br/>Logistic regression was developed by statistician <a href="/wiki/David_Cox_(statistician)" title="David Cox (statistician)">David Cox</a> in 1958. The binary logistic regression model has <a href="#Extensions">extensions</a> to more than two levels of the dependent variable: <a href="/wiki/Categorical_variable" title="Categorical variable">categorical</a> outputs with more than two values are modelled by <a href="/wiki/Multinomial_logistic_regression" title="Multinomial logistic regression">multinomial logistic regression</a>, and if the multiple categories are <a href="/wiki/Level_of_measurement#Ordinal_type" title="Level of measurement">ordered</a>, by <a class="mw-redirect" href="/wiki/Ordinal_logistic_regression" title="Ordinal logistic regression">ordinal logistic regression</a>, for example the proportional odds ordinal logistic model. The model itself simply models probability of output in terms of input, and does not perform <a href="/wiki/Statistical_classification" title="Statistical classification">statistical classification</a> (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a <a class="mw-redirect" href="/wiki/Binary_classifier" title="Binary classifier">binary classifier</a>. The coefficients are generally not computed by a closed-form expression, unlike <a class="mw-redirect" href="/wiki/Linear_least_squares_(mathematics)" title="Linear least squares (mathematics)">linear least squares</a>; see <a href="#Model_fitting">§ Model fitting</a>. <b><a href="/wiki/Logistic_regression" title="Logistic regression">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></b> is the process of using <a href="/wiki/Domain_knowledge" title="Domain knowledge">domain knowledge</a> of the data to create <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">features</a> that make <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated <a href="/wiki/Feature_learning" title="Feature learning">feature learning</a>.<br/><br/>Feature engineering is an informal topic, but it is considered essential in applied machine learning. <b><a href="/wiki/Feature_engineering" title="Feature engineering">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random fields</a></b> (<b>CRFs</b>) are a class of <a href="/wiki/Statistical_model" title="Statistical model">statistical modeling method</a> often applied in <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and used for <a href="/wiki/Structured_prediction" title="Structured prediction">structured prediction</a>. CRFs fall into the sequence modeling family. Whereas a discrete <a href="/wiki/Statistical_classification" title="Statistical classification">classifier</a> predicts a label for a single sample without considering "neighboring" samples, a CRF can take context into account; e.g., the linear chain CRF (which is popular in <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>) predicts sequences of labels for sequences of input samples.<br/><br/>CRFs are a type of <a href="/wiki/Discriminative_model" title="Discriminative model">discriminative</a> <a href="/wiki/Markov_random_field" title="Markov random field">undirected</a> <a href="/wiki/Statistical_model" title="Statistical model">probabilistic</a> <a href="/wiki/Graphical_model" title="Graphical model">graphical model</a>. They are used to encode known relationships between observations and construct consistent interpretations and are often used for <a href="/wiki/Sequence_labeling" title="Sequence labeling">labeling</a> or <a href="/wiki/Parsing" title="Parsing">parsing</a> of sequential data, such as natural language processing or <a href="/wiki/Bioinformatics" title="Bioinformatics">biological sequences</a><br/>and in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>.<br/>Specifically, CRFs find applications in POS Tagging, <a href="/wiki/Shallow_parsing" title="Shallow parsing">shallow parsing</a>,<br/><a class="mw-redirect" href="/wiki/Named_entity_recognition" title="Named entity recognition">named entity recognition</a>,<br/><a href="/wiki/Gene_prediction" title="Gene prediction">gene finding</a> and peptide critical functional region finding,<br/>among other tasks, being an alternative to the related <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">hidden Markov models</a> (HMMs). In computer vision, CRFs are often used for object recognition and image segmentation. <b><a href="/wiki/Conditional_random_field" title="Conditional random field">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>, a <b><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural network</a></b> (<b>CNN</b>, or <b>ConvNet</b>) is a class of <a class="mw-redirect" href="/wiki/Deep_neural_network" title="Deep neural network">deep neural networks</a>, most commonly applied to analyzing visual imagery.<br/><br/>CNNs use a variation of <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a> designed to require minimal <a href="/wiki/Data_pre-processing" title="Data pre-processing">preprocessing</a>. They are also known as <b>shift invariant</b> or <b>space invariant artificial neural networks</b> (<b>SIANN</b>), based on their shared-weights architecture and <a class="mw-redirect" href="/wiki/Translation_invariance" title="Translation invariance">translation invariance</a> characteristics. <b><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference</a></b> (<b>TD</b>) <b>learning</b> refers to a class of <a href="/wiki/Model-free_(reinforcement_learning)" title="Model-free (reinforcement learning)">model-free</a> <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a> methods which learn by <a href="/wiki/Bootstrapping_(statistics)" title="Bootstrapping (statistics)">bootstrapping</a> from the current estimate of the value function. These methods sample from the environment, like <a href="/wiki/Monte_Carlo_method" title="Monte Carlo method">Monte Carlo methods</a>, and perform updates based on current estimates, like <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a> methods.<br/><br/>While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known. This is a form of <a href="/wiki/Bootstrapping" title="Bootstrapping">bootstrapping</a>, as illustrated with the following example: <b><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Data_mining" title="Data mining">data mining</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">hierarchical clustering</a></b> (also called <b>hierarchical cluster analysis</b> or <b>HCA</b>) is a method of <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> which seeks to build a <a href="/wiki/Hierarchy" title="Hierarchy">hierarchy</a> of clusters. Strategies for hierarchical clustering generally fall into two types:<ul><li> <b>Agglomerative</b>: This is a "<a href="/wiki/Top-down_and_bottom-up_design" title="Top-down and bottom-up design">bottom-up</a>" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</li><li> <b>Divisive</b>: This is a "<a href="/wiki/Top-down_and_bottom-up_design" title="Top-down and bottom-up design">top-down</a>" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.</li></ul><br/><br/>In general, the merges and splits are determined in a <a href="/wiki/Greedy_algorithm" title="Greedy algorithm">greedy</a> manner. The results of hierarchical clustering are usually presented in a <a href="/wiki/Dendrogram" title="Dendrogram">dendrogram</a>. <b><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></b> (ERM) is a principle in <a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">statistical learning theory</a> which defines a family of <a href="/wiki/Machine_learning" title="Machine learning">learning algorithms</a> and is used to give theoretical bounds on their performance. <b><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, the <b><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-nearest neighbors algorithm</a></b> (<b><i>k</i>-NN</b>) is a <a class="mw-redirect" href="/wiki/Non-parametric_statistics" title="Non-parametric statistics">non-parametric</a> method used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>. In both cases, the input consists of the <i>k</i> closest training examples in the <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature space</a>. The output depends on whether <i>k</i>-NN is used for classification or regression:<br/><br/>:* In <i>k-NN classification</i>, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its <i>k</i> nearest neighbors (<i>k</i> is a positive <a href="/wiki/Integer" title="Integer">integer</a>, typically small). If <i>k</i> = 1, then the object is simply assigned to the class of that single nearest neighbor. <b><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:196px;"><a class="image" href="/wiki/File:Example_of_unlabeled_data_in_semisupervised_learning.png"><img alt="" class="thumbimage" data-file-height="449" data-file-width="388" decoding="async" height="225" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png/194px-Example_of_unlabeled_data_in_semisupervised_learning.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png/291px-Example_of_unlabeled_data_in_semisupervised_learning.png 1.5x, //upload.wikimedia.org/wikipedia/commons/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png 2x" width="194"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Example_of_unlabeled_data_in_semisupervised_learning.png" title="Enlarge"></a></div>An example of the influence of unlabeled data in semi-supervised learning.  The top panel shows a decision boundary we might adopt after seeing only one positive (white circle) and one negative (black circle) example.  The bottom panel shows a decision boundary we might adopt if, in addition to the two labeled examples, we were given a collection of unlabeled data (gray circles).  This could be viewed as performing <a href="/wiki/Cluster_analysis" title="Cluster analysis">clustering</a> and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.</div></div></div><br/><br/><b><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></b> is a class of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> tasks and techniques that also make use of unlabeled <a href="/wiki/Data" title="Data">data</a> for training – typically a small amount of <a href="/wiki/Labeled_data" title="Labeled data">labeled data</a> with a large amount of unlabeled data.  Semi-supervised learning falls between <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> (without any labeled training data) and <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> (with completely labeled training data).  Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy over unsupervised learning (where no data is labeled), but without the time and costs needed for supervised learning (where all data is labeled). The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value.  Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.<br/><br/>As in the supervised learning framework, we are given a set of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle l}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>l</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle l}</annotation>
</semantics>
</math></span><img alt="l" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;"/></span> <a class="mw-redirect" href="/wiki/Independent_identically_distributed" title="Independent identically distributed">independently identically distributed</a> examples <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{1},\dots ,x_{l}\in X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{1},\dots ,x_{l}\in X}</annotation>
</semantics>
</math></span><img alt="x_{1},\dots ,x_{l}\in X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/76da26bfd12e40809f4b2dae37ecca34ad1c825c" style="vertical-align: -0.671ex; width:14.435ex; height:2.509ex;"/></span> with corresponding labels <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle y_{1},\dots ,y_{l}\in Y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>Y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle y_{1},\dots ,y_{l}\in Y}</annotation>
</semantics>
</math></span><img alt="y_{1},\dots ,y_{l}\in Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b376abba952ea1dca784912adb9a3bfd006eb6b" style="vertical-align: -0.671ex; width:13.847ex; height:2.509ex;"/></span>.  Additionally, we are given <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle u}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>u</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle u}</annotation>
</semantics>
</math></span><img alt="u" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3e6bb763d22c20916ed4f0bb6bd49d7470cffd8" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;"/></span> unlabeled examples <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
<mo>+</mo>
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
<mo>+</mo>
<mi>u</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}</annotation>
</semantics>
</math></span><img alt="x_{l+1},\dots ,x_{l+u}\in X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60a05a61c90d36f1a9def946c7dd83e826162b26" style="vertical-align: -0.671ex; width:18.423ex; height:2.509ex;"/></span>.  Semi-supervised learning attempts to make use of this combined information to surpass the <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning. <b><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></b> is a <a href="/wiki/Ensemble_learning" title="Ensemble learning">machine learning ensemble</a> <a class="mw-redirect" href="/wiki/Meta-algorithm" title="Meta-algorithm">meta-algorithm</a> for primarily reducing <a href="/wiki/Supervised_learning#Bias-variance_tradeoff" title="Supervised learning">bias</a>, and also variance in <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by <a href="/wiki/Michael_Kearns_(computer_scientist)" title="Michael Kearns (computer scientist)">Kearns</a> and <a href="/wiki/Leslie_Valiant" title="Leslie Valiant">Valiant</a> (1988, 1989): "Can a set of <b>weak learners</b> create a single <b>strong learner</b>?" A weak learner is defined to be a <a class="mw-redirect" href="/wiki/Classification_(machine_learning)" title="Classification (machine learning)">classifier</a> that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.<br/><br/><a href="/wiki/Robert_Schapire" title="Robert Schapire">Robert Schapire</a>'s affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, most notably leading to the development of boosting. <b><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Read more...</a></b></div>
</div>
</div></li>
</ul></div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div>
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span id="Need_help.3F"></span><span class="mw-headline" id="Need_help?">Need help?</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<p>Do you have a question about Machine learning that you can't find the answer to?
</p><p>Consider asking it at the <a href="/wiki/Wikipedia:Reference_desk" title="Wikipedia:Reference desk">Wikipedia reference desk</a>.
</p>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div></div><div class="flex-columns-column"><div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Selected_images">Selected images</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<style data-mw-deduplicate="TemplateStyles:r886046910">.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div>div>span:nth-child(2){display:none}@media screen and (max-width:720px){.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li:nth-child(n/**/+5){display:none}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow{padding-left:0;padding-right:0;display:flex;flex-wrap:wrap;justify-content:space-around;align-items:flex-start}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li{width:initial!important;margin:0 0.5em}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li>div>div>div{margin:0.5em 0!important}}</style><div class="randomSlideshow-container" style="max-width:100%; margin:-4em auto;"><ul class="gallery mw-gallery-slideshow" data-showthumbnails="">
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Colored_neural_network.svg"><img alt="" data-file-height="356" data-file-width="296" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/100px-Colored_neural_network.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/150px-Colored_neural_network.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/199px-Colored_neural_network.svg.png 2x" width="100"/></a></div></div>
<div class="gallerytext">
<p>An artificial neural network is an interconnected group of nodes, akin to the vast network of <a href="/wiki/Neuron" title="Neuron">neurons</a> in a <a href="/wiki/Brain" title="Brain">brain</a>. Here, each circular node represents an <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neuron</a> and an arrow represents a connection from the output of one artificial neuron to the input of another.
</p>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Svm_max_sep_hyperplane_with_margin.png"><img alt="" data-file-height="862" data-file-width="800" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/111px-Svm_max_sep_hyperplane_with_margin.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/167px-Svm_max_sep_hyperplane_with_margin.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/223px-Svm_max_sep_hyperplane_with_margin.png 2x" width="111"/></a></div></div>
<div class="gallerytext">
<p>A <a class="mw-redirect" href="/wiki/Support_vector_machine" title="Support vector machine">support vector machine</a> is a supervised learning model that divides the data into regions separated by a <a href="/wiki/Linear_classifier" title="Linear classifier">linear boundary</a>. Here, the linear boundary divides the black circles from the white.
</p>
</div>
</div></li>
</ul></div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div>
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Subcategories">Subcategories</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<div class="floatright"><a class="image" href="/wiki/File:C_Puzzle.png" title="Category puzzle"><img alt="Category puzzle" data-file-height="150" data-file-width="150" decoding="async" height="36" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/36px-C_Puzzle.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/54px-C_Puzzle.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/72px-C_Puzzle.png 2x" width="36"/></a></div>
<dl><dd><small>Select [►] to view subcategories</small></dd></dl>
<div class="CategoryTreeTag" data-ct-mode="0" data-ct-options='{"mode":0,"hideprefix":20,"showcount":false,"namespaces":false}'><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-loaded="1" data-ct-state="expanded" data-ct-title="Machine_learning">▼</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></div><div class="CategoryTreeChildren" style="display:block"><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Applied_machine_learning">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Applied_machine_learning" title="Category:Applied machine learning">Applied machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Artificial_neural_networks">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Bayesian_networks" title="Category:Bayesian networks">Bayesian networks</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Classification_algorithms">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Cluster_analysis">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Cluster_analysis" title="Category:Cluster analysis">Cluster analysis</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Computational_learning_theory" title="Category:Computational learning theory">Computational learning theory</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Artificial_intelligence_conferences" title="Category:Artificial intelligence conferences">Artificial intelligence conferences</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Signal_processing_conferences" title="Category:Signal processing conferences">Signal processing conferences</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Data_mining_and_machine_learning_software">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Data_mining_and_machine_learning_software" title="Category:Data mining and machine learning software">Data mining and machine learning software</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Datasets_in_machine_learning">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Datasets_in_machine_learning" title="Category:Datasets in machine learning">Datasets in machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Deep_learning" title="Category:Deep learning">Deep learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Dimension_reduction">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Ensemble_learning" title="Category:Ensemble learning">Ensemble learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Evolutionary_algorithms">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Evolutionary_algorithms" title="Category:Evolutionary algorithms">Evolutionary algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Genetic_programming" title="Category:Genetic programming">Genetic programming</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Inductive_logic_programming" title="Category:Inductive logic programming">Inductive logic programming</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Kernel_methods_for_machine_learning">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Kernel_methods_for_machine_learning" title="Category:Kernel methods for machine learning">Kernel methods for machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Latent_variable_models">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Latent_variable_models" title="Category:Latent variable models">Latent variable models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Learning_in_computer_vision" title="Category:Learning in computer vision">Learning in computer vision</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Log-linear_models" title="Category:Log-linear models">Log-linear models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Loss_functions" title="Category:Loss functions">Loss functions</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Machine_learning_algorithms">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_portal" title="Category:Machine learning portal">Machine learning portal</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_task" title="Category:Machine learning task">Machine learning task</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Markov_models">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Markov_models" title="Category:Markov models">Markov models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Ontology_learning_(computer_science)" title="Category:Ontology learning (computer science)">Ontology learning (computer science)</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_researchers" title="Category:Machine learning researchers">Machine learning researchers</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Semisupervised_learning" title="Category:Semisupervised learning">Semisupervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Statistical_natural_language_processing">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Statistical_natural_language_processing" title="Category:Statistical natural language processing">Statistical natural language processing</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Structured_prediction">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Structured_prediction" title="Category:Structured prediction">Structured prediction</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Supervised_learning" title="Category:Supervised learning">Supervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Support_vector_machines" title="Category:Support vector machines">Support vector machines</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div></div></div></div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div></div></div>
<div style="clear:both; width:100%">
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Associated_Wikimedia"><i>Associated Wikimedia</i></span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<div class="noprint" style="text-align:center;">
<div style="display:inline-block;margin:0 0 0.75em 0">
<p style="margin:0">The following <a href="/wiki/Wikimedia_Foundation" title="Wikimedia Foundation">Wikimedia Foundation</a> sister projects provide more on this subject:</p>
<div style="display:inline-block;display:flex;flex-wrap:wrap;justify-content:center;"><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikibooks.org/wiki/Special:Search/Machine_learning" title="wikibooks:Special:Search/Machine learning">Wikibooks</a></b><br/>
Books<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikibooks.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="300" data-file-width="300" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/25px-Wikibooks-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/38px-Wikibooks-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/50px-Wikibooks-logo.svg.png 2x" width="25"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://commons.wikimedia.org/wiki/Special:Search/Category:Machine_learning" title="commons:Special:Search/Category:Machine learning">Commons</a></b><br/>
Media<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://commons.wikimedia.org/wiki/Special:Search/Category:Machine_learning"><img alt="" data-file-height="1376" data-file-width="1024" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/28px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/37px-Commons-logo.svg.png 2x" width="18"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikinews.org/wiki/Special:Search/Machine_learning" title="wikinews:Special:Search/Machine learning">Wikinews</a></b> <br/>
News<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikinews.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="415" data-file-width="759" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/46px-Wikinews-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/70px-Wikinews-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/92px-Wikinews-logo.svg.png 2x" width="46"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikiquote.org/wiki/Special:Search/Machine_learning" title="wikiquote:Special:Search/Machine learning">Wikiquote</a></b> <br/>
Quotations<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikiquote.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="355" data-file-width="300" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/21px-Wikiquote-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/32px-Wikiquote-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/42px-Wikiquote-logo.svg.png 2x" width="21"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikisource.org/wiki/Special:Search/Machine_learning" title="wikisource:Special:Search/Machine learning">Wikisource</a></b> <br/>
Texts<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikisource.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="430" data-file-width="410" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/24px-Wikisource-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/36px-Wikisource-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/48px-Wikisource-logo.svg.png 2x" width="24"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikiversity.org/wiki/Special:Search/Machine_learning" title="wikiversity:Special:Search/Machine learning">Wikiversity</a></b><br/>
Learning resources<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikiversity.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="800" data-file-width="1000" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/31px-Wikiversity-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/48px-Wikiversity-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/63px-Wikiversity-logo.svg.png 2x" width="31"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wiktionary.org/wiki/Special:Search/Machine_learning" title="wiktionary:Special:Search/Machine learning">Wiktionary</a></b> <br/>
Definitions<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wiktionary.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="391" data-file-width="391" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/25px-Wiktionary-logo-v2.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/38px-Wiktionary-logo-v2.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/50px-Wiktionary-logo-v2.svg.png 2x" width="25"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://www.wikidata.org/wiki/Special:Search/Machine_learning" title="wikidata:Special:Search/Machine learning">Wikidata</a></b> <br/>
Database<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://www.wikidata.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="590" data-file-width="1050" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/45px-Wikidata-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/68px-Wikidata-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/89px-Wikidata-logo.svg.png 2x" width="45"/></a></div></div>
</div>
</div>
</div>
</div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div>
</div>
<div class="hlist noprint" style="text-align: center; clear:both; padding:0.25em 0 0.5em;">
<ul><li><b>What are <a href="/wiki/Wikipedia:Portal" title="Wikipedia:Portal">portals</a>?</b></li>
<li><b><a href="/wiki/Portal:Contents/Portals" title="Portal:Contents/Portals">List of portals</a></b></li></ul>
</div>
<p><i class="noprint plainlinks"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;action=purge"><small>Purge server cache</small></a></i>
</p></div>
<!-- 
NewPP limit report
Parsed by mw1270
Cached time: 20190315105552
Cache expiry: 21600
Dynamic content: true
CPU time usage: 3.760 seconds
Real time usage: 4.156 seconds
Preprocessor visited node count: 955/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 43701/2097152 bytes
Template argument size: 595/2097152 bytes
Highest expansion depth: 10/40
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 92285/5000000 bytes
Number of Wikibase entities loaded: 0/400
Lua time usage: 3.532/10.000 seconds
Lua memory usage: 4.11 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 3953.789      1 -total
 95.84% 3789.127      1 Template:Flex_columns
 43.13% 1705.191      1 Template:Transclude_selected_recent_additions
 26.85% 1061.453      1 Template:Transclude_list_item_excerpts_as_random_slideshow
 15.36%  607.477      1 Template:Transclude_files_as_random_slideshow
  9.28%  366.851      1 Template:Transclude_selected_current_events
  1.43%   56.653      8 Template:Box-header_colour
  1.13%   44.666      1 Template:Portal_description
  1.07%   42.206      1 Template:Short_description
  0.99%   39.081      1 Template:Pagetype
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:44942806-0!canonical!math=5 and timestamp 20190315105548 and revision id 887475509
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript></div> <div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;oldid=887475509">https://en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;oldid=887475509</a>"					</div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li><li><a href="/wiki/Category:Computing_portals" title="Category:Computing portals">Computing portals</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:Portals_with_short_description" title="Category:Portals with short description">Portals with short description</a></li><li><a href="/wiki/Category:Single-page_portals" title="Category:Single-page portals">Single-page portals</a></li><li><a href="/wiki/Category:All_portals" title="Category:All portals">All portals</a></li><li><a href="/wiki/Category:Portals_with_titles_not_starting_with_a_proper_noun" title="Category:Portals with titles not starting with a proper noun">Portals with titles not starting with a proper noun</a></li></ul></div></div> <div class="visualClear"></div>
</div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<div aria-labelledby="p-personal-label" id="p-personal" role="navigation">
<h3 id="p-personal-label">Personal tools</h3>
<ul>
<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Portal%3AMachine+learning" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Portal%3AMachine+learning" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li> </ul>
</div>
<div id="left-navigation">
<div aria-labelledby="p-namespaces-label" class="vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">Namespaces</h3>
<ul>
<li class="selected" id="ca-nstab-portal"><span><a href="/wiki/Portal:Machine_learning">Portal</a></span></li><li id="ca-talk"><span><a accesskey="t" href="/wiki/Portal_talk:Machine_learning" rel="discussion" title="Discussion about the content page [t]">Talk</a></span></li> </ul>
</div>
<div aria-labelledby="p-variants-label" class="vectorMenu emptyPortlet" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<ul class="menu">
</ul>
</div>
</div>
<div id="right-navigation">
<div aria-labelledby="p-views-label" class="vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">Views</h3>
<ul>
<li class="collapsible selected" id="ca-view"><span><a href="/wiki/Portal:Machine_learning">Read</a></span></li><li class="collapsible" id="ca-edit"><span><a accesskey="e" href="/w/index.php?title=Portal:Machine_learning&amp;action=edit" title="Edit this page [e]">Edit</a></span></li><li class="collapsible" id="ca-history"><span><a accesskey="h" href="/w/index.php?title=Portal:Machine_learning&amp;action=history" title="Past revisions of this page [h]">View history</a></span></li> </ul>
</div>
<div aria-labelledby="p-cactions-label" class="vectorMenu emptyPortlet" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label"><span>More</span></h3>
<ul class="menu">
</ul>
</div>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/><input name="title" type="hidden" value="Special:Search"/><input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search"/><input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/> </div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
<div aria-labelledby="p-navigation-label" class="portal" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">Navigation</h3>
<div class="body">
<ul>
<li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Load a random article [x]">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-interaction-label" class="portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">Interaction</h3>
<div class="body">
<ul>
<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-tb-label" class="portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">Tools</h3>
<div class="body">
<ul>
<li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Portal:Machine_learning" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Portal:Machine_learning" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Portal:Machine_learning&amp;oldid=887475509" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Portal:Machine_learning&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q58630879" title="Link to connected data repository item [g]">Wikidata item</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-coll-print_export-label" class="portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">Print/export</h3>
<div class="body">
<ul>
<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Portal%3AMachine+learning">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Portal%3AMachine+learning&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Portal:Machine_learning&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-lang-label" class="portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">Languages</h3>
<div class="body">
<ul>
<li class="interlanguage-link interwiki-ar"><a class="interlanguage-link-target" href="https://ar.wikipedia.org/wiki/%D8%A8%D9%88%D8%A7%D8%A8%D8%A9:%D8%A7%D9%84%D8%AA%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A2%D9%84%D9%8A" hreflang="ar" lang="ar" title="بوابة:التعلم الآلي – Arabic">العربية</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/Portail:Donn%C3%A9es/Machine_Learning" hreflang="fr" lang="fr" title="Portail:Données/Machine Learning – French">Français</a></li> </ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q58630879#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div> </div>
</div>
</div>
</div>
<div id="footer" role="contentinfo">
<ul id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 12 March 2019, at 22:21<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico">
<a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88"/></a> </li>
<li id="footer-poweredbyico">
<a href="//www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" src="/static/images/poweredby_mediawiki_88x31.png" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88"/></a> </li>
</ul>
<div style="clear: both;"></div>
</div>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"3.760","walltime":"4.156","ppvisitednodes":{"value":955,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":43701,"limit":2097152},"templateargumentsize":{"value":595,"limit":2097152},"expansiondepth":{"value":10,"limit":40},"expensivefunctioncount":{"value":1,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":92285,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00% 3953.789      1 -total"," 95.84% 3789.127      1 Template:Flex_columns"," 43.13% 1705.191      1 Template:Transclude_selected_recent_additions"," 26.85% 1061.453      1 Template:Transclude_list_item_excerpts_as_random_slideshow"," 15.36%  607.477      1 Template:Transclude_files_as_random_slideshow","  9.28%  366.851      1 Template:Transclude_selected_current_events","  1.43%   56.653      8 Template:Box-header_colour","  1.13%   44.666      1 Template:Portal_description","  1.07%   42.206      1 Template:Short_description","  0.99%   39.081      1 Template:Pagetype"]},"scribunto":{"limitreport-timeusage":{"value":"3.532","limit":"10.000"},"limitreport-memusage":{"value":4307198,"limit":52428800}},"cachereport":{"origin":"mw1270","timestamp":"20190315105552","ttl":21600,"transientcontent":true}}});mw.config.set({"wgBackendResponseTime":4284,"wgHostname":"mw1270"});});</script>
</body>
</html>
